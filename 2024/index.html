<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">

        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>UCSD HDSI-TILOS "LLM Meets Theory" Workshop 2024</title>
        <meta name="description" content="Welcome to the UCSD HDSI-TILOS LLM Meets Theory Workshop 2024!">
        <meta name="keywords" content="UCSD,HDSI,TILOS,LLM,Theory,Workshop">
        <meta name="author" content="UCSD HDSI-TILOS LLM Meets Theory Workshop Organizers">

        <link rel="icon" href="static/favicon.ico">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/fonts.css">
        <link rel="stylesheet" href="css/custom.css">
    </head>

    <body>
        <div id="header">
            <div class="container">
               <div id="header-desc">
                   <p style="font-size:2.2rem;text-align:center;">UCSD HDSI-TILOS "LLM Meets Theory" Workshop 2024</p>
                   <p style="font-size:1.3rem;text-align:center;">University of California, San Diego | March 15 to 16, 2024</p>
               </div>
            </div>
        </div>

        <div class="container">
            <nav class="navbar navbar-expand-lg navbar-light bg-light rounded">
                <a class="navbar-brand" href="index.html">Welcome</a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#content" aria-controls="navbarsExample09" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse" id="content">
                    <ul class="navbar-nav mr-auto">
                        <li class="nav-item">
                            <a class="nav-link" href="#schedule"><b>Schedule</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#speaker"><b>Invited Speakers</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#organizer"><b>Organizers</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#sponsor"><b>Sponsors</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#venue"><b>Venue</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href='#contact'><b>Contact</b></a>
                        </li>
                    </ul>
                </div>
            </nav>
        </div>
        <div id="welcome" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Welcome</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>
                        In the last few years we have witnessed an AI miracle. The power and  flexibility of these statistical models far exceeded all expectations of  just a few years a go. It is not a miracle that breaks the laws of nature (that would, of course, be impossible), but it recasts our understanding of data, statistics, and, indeed, human intelligence in a new light.
                    </p>
                    <p>
                        These AI models are relatively simple mathematical formulas. In fact, standard LLMs are essentially Markov chains, and are limited to only predicting the next token. Yet, we do not understand how, why and when they work. In view of this, it is crucial to gain some understanding of the implications and potential trajectory of these models.
                    </p>
                    <p>
                        The UCSD HDSI-TILOS "LLM Meets Theory" Workshop will bring together prominent researchers to discuss the future of mathematical and scientific theory and large language models (LLMs).
                    </p>
                    <h5>Registration</h5>
                    <p>
                        Please fill out <a href="https://forms.gle/L4Sv29usJMZ3tNDs6">this registration form</a>. The workshop is <b>FREE</b>; The knowledge is priceless. 
                    </p>
                    <h5>Important Dates</h5>
                    <p>
                        <b>Workshop date</b>: March 15 to 16, 2024<br>
                    </p>
                </div>
            </div>
        </div>

        <br/>
	 
        <div id="schedule" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Schedule</h4>
                    <hr>
                </div>
            </div>
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
              <h5>Friday, March 15</h5>
              <table>
                  <tr><td><font class="uci-blue">08:00 - 08:45</font>&nbsp;</td> <td><i>Arrival and Light Breakfast</i></td></tr>
                  <tr><td><font class="uci-blue">08:45 - 09:00</font>&nbsp;</td> <td><i>Opening Remarks</i> by <b>Rajesh Gupta</b> (HDSI), <b>Yusu Wang</b> (TILOS), and <b>Misha Belkin</b></i></td></tr>
                  <tr><td><font class="uci-blue">09:00 - 10:00</font>&nbsp;</td> <td><b>Terry Sejnowski (The Salk Institute): <br/><a href="#Terry">Brains and AI</a></b></td></tr>
                  <tr><td><font class="uci-blue">10:00 - 11:00</font>&nbsp;</td> <td><b>Heng Ji (UIUC, Amazon Scholar): <br/><a href="#Heng">LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models</a></b></td></tr>
                  <tr><td><font class="uci-blue">11:00 - 11:30</font>&nbsp;</td> <td><i>Break</i></td></tr>
                  <tr><td><font class="uci-blue">11:30 - 12:30</font>&nbsp;</td> <td><b>Pierre Baldi (UCI): <br/><a href="#Pierre">The Quarks of Attention</a></b></td></tr>
                  <tr><td><font class="uci-blue">12:30 - 02:00</font>&nbsp;</td> <td><i>Lunch Break</i></td></tr>
                  <tr><td><font class="uci-blue">02:00 - 03:00</font>&nbsp;</td> <td><b>Sébastien Bubeck (Microsoft Research): <br/><a href="#Seb">Small Language Models</a></b></td></tr>
                  <tr><td><font class="uci-blue">03:00 - 04:00</font>&nbsp;</td> <td><b>Mahdi Soltanolkotabi (USC)<br/><a href="#Mahdi">Feature Learning in Simple Neural Networks and Prompt-tuned Transformers</a></b></td></tr>
                  <tr><td><font class="uci-blue">04:00 - 05:00</font>&nbsp;</td> <td><b>Panel Discussion: Heng Ji, Pierre Baldi, Sébastien Bubeck, and Mahdi Soltanolkotabi</b>, Moderator: <b>Sanjoy Dasgupta (UCSD)</b></td></tr>
              </table>
            </div>
            <br/>
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
              <h5>Saturday, March 16</h5>
              <table>
                  <tr><td><font class="uci-blue">09:30 - 10:00</font>&nbsp;</td> <td><i>Arrival</i></td></tr>
                  <!-- <tr><td><font class="uci-blue">09:00 - 10:00</font>&nbsp;</td> <td><b>David Donoho (Stanford): <br/><a href="#Donoho">TBD</a></b></td></tr> -->
                  <tr><td><font class="uci-blue">10:00 - 11:00</font>&nbsp;</td> <td><b>Nanyun (Violet) Peng (UCLA): <br/><a href="#Violet">Towards Empowering Large Language Models with Creativity</a></b></td></tr>
                  <!-- <tr><td><font class="uci-blue">11:00 - 11:30</font>&nbsp;</td> <td><i>Break</i></td></tr> -->
                  <tr><td><font class="uci-blue">11:00 - 12:00</font>&nbsp;</td> <td><b>Dan Roth (Amazon AWS AI / UPenn): <br/><a href="#Dan">Reasoning Myths about Language Models: What is Next?</a></b></td></tr>
                  <tr><td><font class="uci-blue">12:00 - 01:30</font>&nbsp;</td> <td><i>Lunch Break</i></td></tr>
                  <tr><td><font class="uci-blue">01:30 - 02:30</font>&nbsp;</td> <td><b>David Danks (UCSD): <br/><a href="#Danks">The impossibility of governing (current) LLMs</a></b></td></tr>
                  <tr><td><font class="uci-blue">02:30 - 03:30</font>&nbsp;</td> <td><b>Panel Discussion: Terry Sejnowski, Nanyun (Violet) Peng, Dan Roth, and David Danks</b>, Moderator: <b>Jingbo Shang (UCSD)</b></td></tr>
              </table>
            </div>
        </div>

        <br/>
		
        <div id="speaker" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Invited Speakers</h4> (alphabetically ordered)
                    <hr>
                </div>
            </div>
			
            <div class="row">
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://www.igb.uci.edu/~pfbaldi/" target="_block">
                          <img src="https://dev.skincenter.uci.edu/People/img/PierreBaldi.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://www.igb.uci.edu/~pfbaldi/" target="_block">Pierre Baldi</a></b></p>
                  <p class="text-center caption caption-role"><b>Distinguished Professor</b></p>
                  <p class="text-center caption caption-role"><b>Information and Computer Sciences</b></p>
                  <p class="text-center caption caption-loc"><i>UCI</i></p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="http://sbubeck.com/" target="_block">
                          <img src="https://www.microsoft.com/en-us/research/uploads/prod/2021/11/test-61a5b079a5601.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="http://sbubeck.com/" target="_block">Sébastien Bubeck</a></b></p>
                  <p class="text-center caption caption-role"><b>Sr. Principal Research Manager</b></p>
                  <p class="text-center caption caption-role"><b>Machine Learning Foundations</b></p>
                  <p class="text-center caption caption-loc"><i>Microsoft Research</i></p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://www.daviddanks.org/" target="_block">
                          <img src="https://omds.ucsd.edu/sites/omds.ucsd.edu/files/img/profile_pictures/David%20Danks%20Web.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://www.daviddanks.org/" target="_block">David Danks</a></b></p>
                  <p class="text-center caption caption-role"><b>Professor</b></p>
                  <p class="text-center caption caption-role"><b>HDSI & Philosophy</b></p>
                  <p class="text-center caption caption-loc"><i>UCSD</i></p>
              </div>
            </div>
            <br/>
            <div class="row">
                <!-- <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://statistics.stanford.edu/people/david-donoho" target="_block">
                          <img src="https://mrc.stanford.edu/sites/mrc/files/styles/responsive_large/public/media/visitors/donoho.jpg?itok=v53drw6G" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://statistics.stanford.edu/people/david-donoho" target="_block">David Donoho</a></b></p>
                  <p class="text-center caption caption-role"><b>Anne T. and Robert M. Bass Professor</b></p>
                  <p class="text-center caption caption-role"><b>Humanities and Sciences & Statistics</b></p>
                  <p class="text-center caption caption-loc"><i>Stanford</i></p>
                </div> -->

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://blender.cs.illinois.edu/hengji.html" target="_block">
                          <img src="https://ws.engr.illinois.edu/directory/viewphoto.aspx?photo=16852&s=300" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://blender.cs.illinois.edu/hengji.html" target="_block">Heng Ji</a></b></p>
                  <p class="text-center caption caption-role"><b>Professor</b></p>
                  <p class="text-center caption caption-role"><b>Computer Science</b></p>
                  <p class="text-center caption caption-loc"><i>UIUC, Amazon Scholar</i></p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://vnpeng.net/" target="_block">
                          <img src="https://vnpeng.net/photos/profile22.png" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://vnpeng.net/" target="_block">Nanyun (Violet) Peng</a></b></p>
                  <p class="text-center caption caption-role"><b>Assistant Professor</b></p>
                  <p class="text-center caption caption-role"><b>Computer Science</b></p>
                  <p class="text-center caption caption-loc"><i>UCLA</i></p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://www.cis.upenn.edu/~danroth/" target="_block">
                          <img src="https://directory.seas.upenn.edu/wp-content/uploads/2020/03/Roth-Dan.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://www.cis.upenn.edu/~danroth/" target="_block">Dan Roth</a></b></p>
                  <p class="text-center caption caption-role"><b>Eduardo D. Glandt Distinguished Professor</b></p>
                  <p class="text-center caption caption-role"><b>Computer Science</b></p>
                  <p class="text-center caption caption-loc"><i>Amazon AWS AI / UPenn</i></p>
                </div>
            </div>
            <br/>
            <div class="row">
		        <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://www.salk.edu/scientist/terrence-sejnowski/" target="_block">
                          <img src="https://www.salk.edu/images/pressrelease/2015/2121-Terry-Sejnowski.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://www.salk.edu/scientist/terrence-sejnowski/" target="_block">Terry Sejnowski</a></b></p>
                  <p class="text-center caption caption-role"><b>Francis Crick Chair</b></p>
                  <p class="text-center caption caption-loc"><i>The Salk Institute</i></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <headshot style="margin: auto;" class="rounded">
                      <a href="https://viterbi-web.usc.edu/~soltanol/" target="_block">
                          <img src="static/mahdiS.jpeg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </headshot>
                  <p class="text-center caption"><b><a href="https://viterbi-web.usc.edu/~soltanol/" target="_block">Mahdi Soltanolkotabi</a></b></p>
                  <p class="text-center caption caption-role"><b>Associate Professor</b></p>
                  <p class="text-center caption caption-role"><b>ECE and Computer Science</b></p>
                  <p class="text-center caption caption-loc"><i>USC</i></p>
                </div>
            </div>

            <br></br>

            <details>
                <summary><b>Click to see the talk and speaker details</b></summary>
                <p>
                    <h5 id="Terry">Terry Sejnowski</h5>
                    <b>Title:</b> Brains and AI<br/>
                    <b>Abstract:</b> The talk will focus on state space models for transformers.<br/>
                    <b>Bio:</b> Terrence Sejnowski is a pioneer in computational neuroscience and his goal is to understand the principles that link brain to behavior. His laboratory uses both experimental and modeling techniques to study the biophysical properties of synapses and neurons and the population dynamics of large networks of neurons. New computational models and new analytical tools have been developed to understand how the brain represents the world and how new representations are formed through learning algorithms for changing the synaptic strengths of connections between neurons. He has published over 300 scientific papers and 12 books, including The Computational Brain, with Patricia Churchland.<br/><br/><br/>

                    <h5 id="Heng">Heng Ji</h5>
                    <b>Title:</b> LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models<br/>
                    <b>Abstract:</b> Today's large language models (LLMs) typically train on short text segments (e.g., <4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues. Through theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis further reveals that commonly used techniques like truncating the attention window or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-infinite brings substantial efficiency improvements: it achieves 2.7 times$ decoding speed up and 7.5 times  memory saving over the original model. <br/>
                    <b>Bio:</b> Heng Ji is a professor at Computer Science Department, and an affiliated faculty member at Electrical and Computer Engineering Department and Coordinated Science Laboratory of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). She received her B.A. and M. A. in Computational Linguistics from Tsinghua University, and her M.S. and Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models, Knowledge-driven Generation and Conversational AI. She was selected as a Young Scientist to attend the 6th World Laureates Association Forum, and selected to participate in DARPA AI Forward in 2023. She was selected as "Young Scientist" and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017. She was named as part of Women Leaders of Conversational AI (Class of 2023) by Project Voice. The other awards she received include "AI's 10 to Watch" Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, "Best of ICDM2013" paper award, "Best of SDM2013" paper award, ACL2018 Best Demo paper nomination, ACL2020 Best Demo Paper Award, NAACL2021 Best Demo Paper Award, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She was invited to testify to the U.S. House Cybersecurity, Data Analytics, & IT Committee as an AI expert in 2023. She was invited by the Secretary of the U.S. Air Force and AFRL to join Air Force Data Analytics Expert Panel to inform the Air Force Strategy 2030, and invited to speak at the Federal Information Integrity R&D Interagency Working Group (IIRD IWG) briefing in 2023. She has coordinated the NIST TAC Knowledge Base Population task since 2010. She was the associate editor for IEEE/ACM Transaction on Audio, Speech, and Language Processing, and served as the Program Committee Co-Chair of many conferences including NAACL-HLT2018 and AACL-IJCNLP2022. She is elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023. <br/><br/><br/>

                    <h5 id="Pierre">Pierre Baldi</h5>
                    <b>Title:</b> The Quarks of Attention<br/>
                    <b>Abstract:</b> Attention plays a fundamental role in both natural and artificial intelligence systems. In deep learning, several attention-based neural network architectures have been proposed to tackle problems in natural language processing (NLP)  and beyond, including transformer architectures which currently achieve state-of-the-art performance in NLP tasks and power modern large language models (LLMs) In this presentation we will: 1) identify and classify the most fundamental building blocks (quarks) of attention, both within and beyond the standard model of deep learning; 2) identify how these building blocks are used in all current attention-based architectures, including transformers; 3) demonstrate how transformers can effectively be applied to new problems in physics, from particle physics to astronomy; and 4) present a mathematical theory of attention capacity where, paradoxically, one of the main tools in the proofs is itself an attention mechanism. We will showcase some recent applications of LLMs and discuss open questions.<br/>
                    <b>Bio:</b> Pierre Baldi earned MS degrees in Mathematics and Psychology from the University of Paris, and a PhD in Mathematics from the California Institute of Technology. He is currently Distinguished Professor in the Department of Computer Science, Director of the Institute for Genomics and Bioinformatics, and Associate Director of the Center for Machine Learning and Intelligent Systems  at the University of California Irvine. The long term focus of his research is on understanding intelligence in brains and machines. He has made several contributions to the theory of AI and deep learning, and developed and applied AI and deep learning methods for the natural sciences, to address problems in  physics (e.g., exotic particle detection) ,  chemistry (e.g., reaction prediction), and bio-medicine (e.g., protein structure prediction, biomedical imaging analysis). He recently published his fifth book: Deep Learning in Science, Cambridge University Press (2021). His honors include the 1993 Lew Allen Award at JPL, the 2010 E. R. Caianiello Prize for research in machine learning, and election to Fellow of the AAAS, AAAI, IEEE, ACM, and ISCB. He has also co-founded several startup companies.<br/><br/><br/>
                    
                    <h5 id="Seb">Sébastien Bubeck</h5>
                    <b>Title:</b> Small Language Models<br/>
                    <b>Abstract:</b> How far can you go with a 1B parameters transformer? Turns out, pretty far if you feed it the right data.<br/>
                    <b>Bio:</b> Seb Bubeck used to spend his time thinking about the theory of machine learning. With the advent of miraculous LLMs, he switched to building LLMs to get some insights into how the miracle happens.<br/><br/><br/>

                    <h5 id="Mahdi">Mahdi Soltanolkotabi</h5>
                    <b>Title:</b> Feature Learning in Simple Neural Networks and Prompt-tuned Transformers<br/>
                    <b>Abstract:</b> One of the major transformations in modern learning is that contemporary models trained through gradient descent have the ability to learn versatile representations that can then be applied effectively across a broad range of down-stream tasks. Existing theory however suggests that neural networks, when trained via gradient descent, behave similar to kernel methods that fail to learn representations that can be transferred. In the first part of this talk I will try to bridge this discrepancy by showing that gradient descent on neural networks can indeed learn a broad spectrum of functions that kernel methods struggle with, by acquiring task-relevant representations. In the second part of the talk I will focus on feature learning in prompt-tuning which is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. We demystify how prompt-tuning enables the model to focus attention to context-relevant information/features.<br/>
                    <b>Bio:</b> Mahdi Soltanolkotabi is the director of the center on AI Foundations for the Sciences (AIF4S) at the University of Southern California. He is also an associate professor in the Departments of Electrical and Computer Engineering, Computer Science, and Industrial and Systems engineering where he holds an Andrew and Erna Viterbi Early Career Chair. Prior to joining USC, he completed his PhD in electrical engineering at Stanford in 2014. He was a postdoctoral researcher in the EECS department at UC Berkeley during the 2014-2015 academic year. Mahdi is the recipient of the Information Theory Society Best Paper Award, Packard Fellowship in Science and Engineering, an NIH Director’s new innovator award, a Sloan Research Fellowship, an NSF Career award, an Airforce Office of Research Young Investigator award (AFOSR-YIP), the Viterbi school of engineering junior faculty research award, and faculty awards from Google and Amazon. His research focuses on developing the mathematical foundations of modern data science via characterizing the behavior and pitfalls of contemporary nonconvex learning and optimization algorithms with applications in deep learning, large scale distributed training, federated learning, computational imaging, and AI for scientific and medical applications.<br/><br/><br/>

                    <!-- <h5 id="Donoho">David Donoho</h5>
                    <b>Title:</b> TBD<br/>
                    <b>Abstract:</b> TBD<br/>
                    <b>Bio:</b> TBD<br/><br/><br/> -->


                    <h5 id="Violet">Nanyun (Violet) Peng</h5>
                    <b>Title:</b> Towards Empowering Large Language Models with Creativity<br/>
                    <b>Abstract:</b> Recent advances in large auto-regressive language models have demonstrated strong results in generating natural languages and significantly improved the performances for applications such as dialogue systems, machine translation, and summarization. However, the auto-regressive paradigm trains models to capture the surface patterns (i.e. sequences of words) following the left-to-right order, instead of capturing underlying semantics and discourse structures. It is also hard to impose structural or content control/contraints to the model. In this talk, I will present our recent works on controllable natural language generation that go beyond the prevalent auto-regressive formulation with the goal to improve controllability and creativity. We propose novel insertion-based generation models and controllable decoding-time algorithms to steer models to better conform to constraints, with applications to creative poetry generation, lyric generation, and keyword-to-text generation.<br/>
                    <b>Bio:</b> Nanyun (Violet) Peng is an Assistant Professor of Computer Science at the University of California, Los Angeles. She received her Ph.D. in Computer Science from Johns Hopkins University, Center for Language and Speech Processing. Her research focuses on the generalizability of NLP models, with applications to creative language generation, low-resource information extraction, and zero-shot cross-lingual transfer. Her works have won the Outstanding Paper Award at NAACL 2022, the Best Paper Awards at AAAI 2022 DLG workshop, EMNLP 2023 PAN-DL workshop, and have been featured at the IJCAI 2022 early career spotlight. Her research has been supported by NSF, DARPA, IARPA, NIH grants and various industrial faculty awards.<br/><br/><br/>


                    <h5 id="Dan">Dan Roth</h5>
                    <b>Title:</b> Reasoning Myths about Language Models: What is Next?<br/>
                    <b>Abstract:</b> The rapid progress made over the last few years in generating linguistically coherent natural language has blurred, in the mind of many, the difference between natural language generation, understanding, and the ability to reason with respect to the world. Nevertheless, robust support of high-level decisions that depend on natural language understanding, and one that requires dealing with “truthfulness” are still beyond our capabilities, partly since most of these tasks are very sparse, often require grounding, and may depend on new types of supervision signals. I will discuss some of the challenges underlying reasoning and argue that we should focus on LLMs as orchestrators – coordinating and managing multiple models, applications, and services, as a way to execute complex tasks and processes. I will discuss some of the challenges and present some of our work in this space, focusing on supporting task decomposition and planning.<br/>
                    <b>Bio:</b> Dan Roth is the Eduardo D. Glandt Distinguished Professor at the Department of Computer and Information Science, University of Pennsylvania, a VP/Distinguished Scientist at AWS AI, and a Fellow of the AAAS, the ACM, AAAI, and the ACL. In 2017 Roth was awarded the John McCarthy Award, the highest award the AI community gives to mid-career AI researchers. Roth was recognized “for major conceptual and theoretical advances in the modeling of natural language understanding, machine learning, and reasoning.” Roth has published broadly in machine learning, natural language processing, knowledge representation and reasoning, and learning theory. He was the Editor-in-Chief of the Journal of Artificial Intelligence Research (JAIR), has served as the Program Chair for AAAI, ACL and CoNLL, and as a Conference Chair for a few top conferences. Roth has been involved in several startups; most recently he was a co-founder and chief scientist of NexLP, a startup that leverages the latest advances in Natural Language Processing (NLP), Cognitive Analytics, and Machine Learning in the legal and compliance domains. NexLP was acquired by Reveal in 2020. Prof. Roth received his B.A Summa cum laude in Mathematics from the Technion, Israel, and his Ph.D. in Computer Science from Harvard University in 1995.<br/><br/><br/>

                    <h5 id="Danks">David Danks</h5>
                    <b>Title:</b> The impossibility of governing (current) LLMs<br/>
                    <b>Abstract:</b> LLMs are spreading rapidly across all sectors, resulting in many calls to govern them, whether for safety, public benefit, economic gains, or some other goal. In this talk, I will argue that the standard tools of governance (regulation, post hoc testing, and so forth) will always be inadequate for LLMs as we currently understand them. That is, LLMs simply cannot be governed in the usual ways, at least not without massive changes. I will suggest what some of those changes might be, and also explore how we might move towards more governable LLMs.<br/>
                    <b>Bio:</b> David Danks is Professor of Data Science & Philosophy at University of California, San Diego. He both builds AI systems, and also examines the ethical and policy issues around AI and robotics.<br/><br/><br/>
                </p>
            </details>
        </div>
        <br/>

        <div id="organizer" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Organizers</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <headshot style="margin: auto;" class="rounded">
                       <a href="http://misha.belkin-wang.org/" target="_block">
                           <img src="https://datascience.ucsd.edu/wp-content/uploads/2022/09/Misha-Belkin.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </headshot>
                    <p class="text-center caption"><b><a href="http://misha.belkin-wang.org/" target="_block">Mikhail Belkin</a></b></p>
                    <p class="text-center caption caption-role"><b>Professor</b></p>
                    <p class="text-center caption caption-role"><b>HDSI</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <headshot style="margin: auto;" class="rounded">
                       <a href="https://shangjingbo1226.github.io/" target="_block">
                           <img src="https://shangjingbo1226.github.io/images//img/avatar.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </headshot>
                    <p class="text-center caption"><b><a href="https://shangjingbo1226.github.io/" target="_block">Jingbo Shang</a></b></p>
                    <p class="text-center caption caption-role"><b>Assistant Professor</b></p>
                    <p class="text-center caption caption-role"><b>Computer Science and HDSI</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
            </div>
        </div>

        <br/>

        <div id="sponsor" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Sponsors </h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://datascience.ucsd.edu/" target="_block"><img src="static/UCSanDiego_Halicioglu_BlueGold.png" class="rounded img-fluid mx-auto d-block" style="height:5rem;"/></a>
                    </div>
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://tilos.ai/" target="_block"><img src="static/TILOS-LOGO.png" class="rounded img-fluid mx-auto d-block" style="height:5rem;"/></a>
                    </div>
                </div>
            </div>
        </div>

        <br/>

        <div id="venue" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Venue</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>
                        <b>Date:</b> March 15 to 16, 2024<br/>
                        <b>Location:</b> <a href="https://maps.app.goo.gl/Bgfpu6JErK9hU1tv5">HDSI Building</a> at UCSD <br/><br/>
                        <iframe src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d13402.662840725787!2d-117.2338242!3d32.8805631!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x80dc06c1fb192bcb%3A0x94d4894527b99b21!2sHal%C4%B1c%C4%B1o%C4%9Flu%20Data%20Science%20Institute!5e0!3m2!1sen!2sus!4v1707368756232!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
                        <br/>
                        <br/>
                        <b>Parking information:</b> Campus parking is very limited and is not recommended. You can check available options here. The <a href="https://maps.app.goo.gl/tC63TM3LR74PUTFG7">Gilman parking structure</a> is nearby. Another close parking lot for visitors is located on the top floor of <a href="https://maps.app.goo.gl/FfjaukxKu9E3Kfaw9">Hopkins Parking Structure</a>. You can pay the fee in pay station or pay-by-phone. Note that most visitor spots are limited to two hours. Even though the app allows you to pay for longer periods, you will get a ticket after that time.
                        <br/>
                        <br/>
                        There is free parking at <a href="https://www.flytorrey.com/">Torrey Pines Gliderport</a>, if you do not mind a 35-minute walk to HDSI. The view from the Gliderport is spectacular.

                        <figure class="wp-block-gallery has-nested-images columns-default is-cropped wp-block-gallery-1 is-layout-flex wp-block-gallery-is-layout-flex">
                        <figure class="wp-block-image size-large">
                            <a href="https://www.flickr.com/photos/ginatrapani/2840540583/">
                            <img fetchpriority="high" decoding="async" width="800" height="600" src="static/glider.jpeg"></a><figcaption class="wp-element-caption">View from the Torrey Pines Gliderport. Photo credit: <a href="https://www.flickr.com/photos/ginatrapani/" title="">Gina Trapani</a> (<a href="https://creativecommons.org/licenses/by-nc-sa/2.0/" title="">CC BY-NC-SA 2.0 Deed</a>)</figcaption>
                        </figure>
                    </figure>
                    </p>
                </div>
            </div>
        </div>
        
        <br/>

        <div id="contact" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Contact</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>
                        Please feel free to forward this message to those who might be interested. If you have questions, please contact the organizers.
                    </p>
                </div>
            </div>
        </div>

        <br/>
        <hr>

        <div class="container footer">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p align="left">
                        Copyright &copy; UCSD HDSI-TILOS "LLM Meets Theory" Workshop 2024<br/>
                        Cover photo from  <a href="https://twitter.com/UCSDalumni/status/1340121401005522944/photo/1" target="_blank">@UCSDalumni tweet | Erik Jepsen</a><br/>
                        Website template adapted from SoCal NLP Symposium. 
                    </p>
                </div>
            </div>
        </div>
    </body>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')</script>
    <script src="js/popper.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
</html>
